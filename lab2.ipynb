{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet101V2\n",
    "from glob import glob\n",
    "import json\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODE = False\n",
    "PATH = \"/disk2/herbarium_data/nybg2020/\"\n",
    "TRAIN = PATH+\"train/\"\n",
    "TEST = PATH+\"test/\"\n",
    "META = \"metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN+META, errors='ignore', encoding='utf8') as f:\n",
    "    meta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta data keys:\n",
      "- annotations\n",
      "- categories\n",
      "- images\n",
      "- info\n",
      "- licenses\n",
      "- regions\n",
      "\n",
      "Sample of annotations:\n",
      "{'category_id': 15672, 'id': 354106, 'image_id': 354106, 'region_id': 1}\n",
      "\n",
      "Sample of categories:\n",
      "{'family': 'Orchidaceae', 'genus': 'Aa', 'id': 0, 'name': 'Aa mathewsii (Rchb.f.) Schltr.'}\n",
      "\n",
      "Sample of images:\n",
      "{'file_name': 'images/156/72/354106.jpg', 'height': 1000, 'id': 354106, 'license': 1, 'width': 661}\n",
      "\n",
      "Sample of info:\n",
      "contributor\n",
      "\n",
      "Sample of licenses:\n",
      "{'id': 1, 'name': 'Public Domain Dedication', 'url': 'http://creativecommons.org/publicdomain/zero/1.0/'}\n",
      "\n",
      "Sample of regions:\n",
      "{'id': 0, 'name': 'Mexico & Central America'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Meta data keys:\")\n",
    "for i in meta.keys():\n",
    "    print(\"- \"+i)\n",
    "for i in meta.keys():\n",
    "    print(\"\\nSample of \"+i+\":\")\n",
    "    print(list(meta[i])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = pd.DataFrame(meta['images'])\n",
    "train_ann = pd.DataFrame(meta['annotations'])\n",
    "train_df = pd.merge(train_ann, train_img, left_on='image_id', right_on='id', how='left').drop('image_id', axis=1)\n",
    "train_df = shuffle(train_df)\n",
    "max_class = train_df['category_id'].max()\n",
    "#train_df['category_id'] = train_df['category_id'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"unique, counts = np.unique(train_df['category_id'], return_counts=True)\n",
    "plt.bar(unique, counts, 1)\n",
    "plt.title('Class Frequency')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Image.open(TRAIN+train_df['file_name'][0])\n",
    "\n",
    "size_of_img = (28, 28)\n",
    "fig=plt.figure(figsize=(72,72))\n",
    "for i in range(60):\n",
    "    ax=fig.add_subplot(12,12,i+1)\n",
    "    img = cv2.imread(TRAIN + meta[\"images\"][i][\"file_name\"])\n",
    "    img = cv2.resize(img,size_of_img)\n",
    "    ax.imshow(img)\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = len(train_df)\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, \n",
    "                                                                  validation_split=0.2,\n",
    "                                                                  horizontal_flip=True,\n",
    "                                                                  zoom_range=0.1)\n",
    "NO_IMPROVE = 5\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VAL_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = image_generator.flow_from_dataframe(dataframe=train_df,\n",
    "                                                     directory=TRAIN,\n",
    "                                                     x_col='file_name',\n",
    "                                                     y_col='category_id',\n",
    "                                                     batch_size=TRAIN_BATCH_SIZE,\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     class_mode='raw',\n",
    "                                                     subset='training')\n",
    "val_data_gen = image_generator.flow_from_dataframe(dataframe=train_df,\n",
    "                                                     directory=TRAIN,\n",
    "                                                     x_col='file_name',\n",
    "                                                     y_col='category_id',\n",
    "                                                     batch_size=VAL_BATCH_SIZE,\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     class_mode='raw',\n",
    "                                                     subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch, label_batch):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for n in range(2):\n",
    "      ax = plt.subplot(5,5,n+1)\n",
    "      plt.imshow(image_batch[n])\n",
    "      plt.axis('off')\n",
    "\n",
    "\"\"\"image_batch, label_batch = next(train_data_gen)\n",
    "show_batch(image_batch, label_batch)\n",
    "print(label_batch)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if TRAIN_MODE:\n",
    "    resnet = ResNet101V2(include_top=True, input_tensor=None, input_shape=(IMG_HEIGHT, IMG_HEIGHT,3))\n",
    "    output = resnet.layers[-2].output\n",
    "    for layer in resnet.layers:\n",
    "        layer.trainable = True\n",
    "    output = tf.keras.layers.Dropout(0.1)(output)\n",
    "    output = tf.keras.layers.BatchNormalization()(output)\n",
    "    output = tf.keras.layers.Dense(max_class+1, activation='softmax')(output)\n",
    "    model = tf.keras.Model(inputs=resnet.input, outputs=output)\n",
    "    stop_when_no_improve = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', min_delta=0, \n",
    "                                                                patience = NO_IMPROVE, restore_best_weights=True)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    print(model.summary())\n",
    "\n",
    "    model.fit(\n",
    "        train_data_gen,\n",
    "        epochs=10000,\n",
    "        validation_data=val_data_gen,\n",
    "        callbacks=[stop_when_no_improve]\n",
    "    )\n",
    "\n",
    "    model.save(\"./model.h5\")\n",
    "else:\n",
    "    model = tf.keras.models.load_model(\"./model_0.17736.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST+META, errors='ignore', encoding='utf8') as f:\n",
    "    meta_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meta data keys:\")\n",
    "for i in meta_test.keys():\n",
    "    print(\"- \"+i)\n",
    "for i in meta_test.keys():\n",
    "    print(\"\\nSample of \"+i+\":\")\n",
    "    print(list(meta_test[i])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = pd.DataFrame(meta_test['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_data_gen = test_image_generator.flow_from_dataframe(dataframe=test_img,\n",
    "                                                         directory=TEST,\n",
    "                                                         x_col='file_name',\n",
    "                                                         batch_size=64,\n",
    "                                                         shuffle=False,\n",
    "                                                         target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                         class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x=test_data_gen, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([test_img['id'], pd.DataFrame(result)], axis=1).rename(columns={\"id\": \"Id\", 0: \"Predicted\"})\n",
    "output.set_index('Id').sort_index().to_csv('./submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
